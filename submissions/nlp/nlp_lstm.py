# -*- coding: utf-8 -*-
"""dicoding-bpml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XwgusU3cASovS9l7AaEOvJpX9zymq-K2

# **Libraries**
"""

import warnings
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Sequential
from sklearn.metrics import confusion_matrix, classification_report

warnings.filterwarnings('ignore')

"""# **Data Preparation**

## **Download Data**
"""

# Because this data (train.txt) has 16k (2k required) text, i use it as main data and then split it to train and test
!wget --no-check-certificate https://raw.githubusercontent.com/zekhoi/dicoding-bpml/master/data/nlp/emotions.txt -O /tmp/data.txt

df = pd.read_csv("/tmp/data.txt", error_bad_lines=False, delimiter=";", names=["text","emotions"])
df

"""## **Initialize**"""

df["emotions"].value_counts()

emotions = pd.get_dummies(df.emotions)
df = pd.concat([df, emotions], axis = 1)
df = df.drop(columns = "emotions")
df

# Splitting data to X and Y
X = df["text"].values
y = df[["anger", "fear", "joy", "love", "sadness", "surprise"]].values

# Splitting data to train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

"""# **Execute**

## **Tokenizing and Padding**
"""

tokenizer = Tokenizer(num_words = 16000, oov_token = "-unknown-")
tokenizer.fit_on_texts(X_train)
num_words = len(tokenizer.word_index) + 1

tokenizer = Tokenizer(num_words = num_words, oov_token = "-unknown-")
tokenizer.fit_on_texts(X_train) 

train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(train_sequences)

test_sequences = tokenizer.texts_to_sequences(X_test)
test_padded = pad_sequences(test_sequences)

"""## **Model**

### **Callbacks**
"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get("accuracy") > 0.9 and logs.get("val_accuracy") > 0.9):
      print("Target reached!")
      self.model.stop_training = True

callbacks = myCallback()

"""### **LSTM**"""

# Buiding LSTM Model
lstm_model = Sequential([
    tf.keras.layers.Embedding(input_dim = num_words, output_dim = 36),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation="relu"),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dense(6, activation="softmax")
])
lstm_model.summary()

"""### **Training**"""

num_epochs = 30

lstm_model.compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = ["accuracy"])
lstm_result = lstm_model.fit(train_padded, y_train, epochs = num_epochs, validation_data = (test_padded, y_test), verbose = 1)

"""## **Result Plot**"""

plt.figure(figsize=(15,7))
plt.plot(lstm_result.history["accuracy"])
plt.plot(lstm_result.history["val_accuracy"])
plt.title("Model Accuracy")
plt.ylabel("accuracy")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc = "upper left")
plt.show()

plt.figure(figsize = (15,7))
plt.plot(lstm_result.history["loss"])
plt.plot(lstm_result.history["val_loss"])
plt.title("Model Loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""## **Try**"""

# Try to predict random sentences
sample = df.sample(n = 5)
emo = ["anger", "fear", "joy", "love", "sadness", "surprise"]
sentences = sample["text"].values
emotions = sample[emo].values
le = LabelEncoder()
le.fit(emo)

print("[anger, fear, joy, love, sadness, surprise] \n\n")
for index, sentence in enumerate(sentences):
    print(sentence)
    sentence = tokenizer.texts_to_sequences([sentence])
    sentence = pad_sequences(sentence)
    result = le.inverse_transform(np.argmax(lstm_model.predict(sentence), axis = -1))[0]
    prob =  np.max(lstm_model.predict(sentence))
    emotion = emo[list(emotions[index]).index(1)]
    print(f"{emotion} => {result} : {prob}\n\n")